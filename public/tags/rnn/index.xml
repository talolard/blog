<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rnn on Tal Perry</title>
    <link>http://localhost:1313/tags/rnn/</link>
    <description>Recent content in Rnn on Tal Perry</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 May 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Convolutional Methods for Text</title>
      <link>http://localhost:1313/posts/classics/cmft/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/classics/cmft/</guid>
      <description>tl;dr RNNS work great for text but convolutions can do it faster Any part of a sentence can influence the semantics of a word. For that reason we want our network to see the entire input at once Getting that big a receptive can make gradients vanish and our networks fail We can solve the vanishing gradient problem with DenseNets or Dilated Convolutions Sometimes we need to generate text. We can use “deconvolutions” to generate arbitrarily long outputs.</description>
    </item>
    <item>
      <title>Deep Learning The Stock Market</title>
      <link>http://localhost:1313/posts/classics/dlsm/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/classics/dlsm/</guid>
      <description>_ Update 15.03.2024 I wrote this more than seven years ago. My understanding has evolved since then, and the world of deep learning has gone through more than one revolution since. It was popular back in the day and might still be a fun read though you might learn more accurate and upto date information somewhere else_&#xA;_ Update 25.1.17 — Took me a while but_ here is an ipython notebook with a rough implementation</description>
    </item>
  </channel>
</rss>
