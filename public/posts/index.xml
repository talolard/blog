<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Tal Perry</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Tal Perry</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Mar 2024 09:41:38 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lighttag</title>
      <link>http://localhost:1313/posts/portfolio/lighttag/</link>
      <pubDate>Fri, 15 Mar 2024 09:41:38 +0100</pubDate>
      <guid>http://localhost:1313/posts/portfolio/lighttag/</guid>
      <description>This is a write up&#xA;def fib(n :int): -&amp;gt; int return n+fib(n-1) </description>
    </item>
    <item>
      <title>My Time With Ritalin</title>
      <link>http://localhost:1313/posts/my-time-with-ritalin/</link>
      <pubDate>Fri, 15 Mar 2024 09:37:34 +0100</pubDate>
      <guid>http://localhost:1313/posts/my-time-with-ritalin/</guid>
      <description>This is a post</description>
    </item>
    <item>
      <title>Convolutional Methods for Text</title>
      <link>http://localhost:1313/posts/classics/cmft/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/classics/cmft/</guid>
      <description>tl;dr RNNS work great for text but convolutions can do it faster Any part of a sentence can influence the semantics of a word. For that reason we want our network to see the entire input at once Getting that big a receptive can make gradients vanish and our networks fail We can solve the vanishing gradient problem with DenseNets or Dilated Convolutions Sometimes we need to generate text. We can use “deconvolutions” to generate arbitrarily long outputs.</description>
    </item>
    <item>
      <title>Deep Learning The Stock Market</title>
      <link>http://localhost:1313/posts/classics/dlsm/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/classics/dlsm/</guid>
      <description>_ Update 15.03.2024 I wrote this more than seven years ago. My understanding has evolved since then, and the world of deep learning has gone through more than one revolution since. It was popular back in the day and might still be a fun read though you might learn more accurate and upto date information somewhere else_&#xA;_ Update 25.1.17 — Took me a while but_ here is an ipython notebook with a rough implementation</description>
    </item>
  </channel>
</rss>
