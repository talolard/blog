# r/mlops — Hot conversations summary (2026-01-12)

Source: `https://old.reddit.com/r/mlops/hot/` (captured 2026-01-12).

## Automating ML pipelines with Airflow (DockerOperator vs mounted project)

Link: `https://old.reddit.com/r/mlops/comments/1qa2sww/automating_ml_pipelines_with_airflow/`

**What it’s about**
- A data scientist building an end-to-end personal pipeline (yfinance → feature engineering → Feast → training → MLflow model registry / champion-challenger → FastAPI serving → Evidently monitoring; plus Docker/MinIO/Airflow).
- They’re unsure how to “automate with Airflow”: build one container for everything + run with `DockerOperator`, or mount the project into Airflow and run tasks from there.

**Reply / addition ideas**
- Treat Airflow as an *orchestrator*, not a monolith runner: define a DAG with small, idempotent tasks (extract → features → train → validate → register → deploy → monitor) that communicate via durable artifacts (S3/MinIO) and metadata (MLflow).
- Containerize *steps* (or a small set of compatible steps), not “the whole repo”: separate images/environments for training, batch feature computation, and serving; keep Airflow image focused on orchestration.
- Prefer an execution backend aligned with deployment target (K8sPodOperator / ECS / batch jobs) so each task runs in its own isolated environment with pinned deps.
- Add practical pipeline hygiene: retries that don’t duplicate side effects, deterministic run IDs, partitioned data paths, backfills, and explicit “promote to champion” gates.

## Cost-efficient high-concurrency multi-model inference (Lambda → SageMaker MME bottleneck)

Link: `https://old.reddit.com/r/mlops/comments/1q4vfg0/need_help_designing_a_cost_efficient_architecture/`

**What it’s about**
- Small TF models (~700KB) loaded per Lambda from S3 into `/tmp`; at higher traffic, they see `/tmp` full and occasional OOM due to warm environment reuse (state persists across invocations).
- Moved inference to SageMaker multi-model endpoint; then hit throughput/latency bottlenecks or rising cost with larger/more instances.
- Target: ~10k requests / 5 minutes (~34 RPS), many different models.

**Reply / addition ideas**
- Confirm: warm Lambda reuse means `/tmp` and process memory persist; each concurrent execution uses separate containers, but containers are reused over time.
- If staying on Lambda: use EFS for shared model cache (or stricter `/tmp` eviction), keep model loading deterministic, and explicitly free resources; but TF memory behavior can still be painful.
- Consider shifting inference to a container pool (ECS/Fargate/K8s) with autoscaling + “worker recycling” to bound memory creep (e.g., restart workers after N requests / time).
- Queue + worker model (SQS) can smooth bursts and improve cost control; pair with per-model caching in the worker process.
- If models are compatible, consider a unified model server (Triton / TF Serving) with dynamic loading and better batching/concurrency controls; also evaluate lighter runtimes (TFLite / ONNX Runtime) for small models.

## “MLOps” terminology confusion (app-level vs platform-level)

Link: `https://old.reddit.com/r/mlops/comments/1q9dlxo/confused_about_terminology_in_this_area/`

**What it’s about**
- Poster asks whether popular “MLOps zoomcamp” content is really “application-level MLOps”, vs what many here call “platform-level MLOps”.

**Reply / addition ideas**
- Offer a compact taxonomy:
  - Product / application MLOps: shipping one/few models into a product (training→deploy→monitor loops).
  - Platform MLOps: shared infra/tooling for many teams (feature store, CI/CD, registry, serving platform, observability, governance).
  - Data platform: ingestion/ETL/lakehouse, quality, lineage—often intertwined.
- Encourage specifying scope by responsibilities/outcomes rather than labels (deploy frequency, incident ownership, serving SLOs, governance/compliance).
- Suggest a “skills matrix” people can map themselves onto (data mgmt, training orchestration, registry, serving, monitoring/eval, security/ACL, cost control).

## Triton inference server practices (Laravel backend + S3 images; where to preprocess?)

Link: `https://old.reddit.com/r/mlops/comments/1q855uo/triton_inference_server_good_practices/`

**What it’s about**
- SaaS building a Triton ensemble: segmentation (SAM3) → inpainting (LaMa). Inputs include an image + prompt + thresholds; output is a final image (bytes).
- Client is a Laravel backend; images live in S3. Question: download+decode/convert to `UINT8` tensors inside Triton (CPU preprocessing step) vs Laravel converts and sends tensors over the network.

**Reply / addition ideas**
- Strong default: avoid sending raw tensors over WAN; send compressed bytes and decode close to the GPU to reduce bandwidth and latency variance.
- If Triton can reach S3 privately (VPC endpoint / IAM), doing a preprocessing step near Triton can be good—otherwise app downloads from S3 and forwards bytes may be simpler.
- Consider Triton-side preprocessing options:
  - Python backend for decode/resize/normalize,
  - NVIDIA DALI for GPU-accelerated decode/augment (when appropriate),
  - enforce width/height caps and input validation to prevent overload.
- Call out operational concerns: request size limits, batching, concurrency, caching, and ACL/signing model for S3 access.

## Practical 2026 roadmap for AI search & RAG systems

Link: `https://old.reddit.com/r/mlops/comments/1q87ytk/a_practical_2026_roadmap_for_modern_ai_search_rag/`

**What it’s about**
- Roadmap arguing real RAG/search goes beyond “vector DB + prompt”: hybrid retrieval, reranking, query understanding, agentic multi-hop, freshness/lifecycle, grounding, evaluation, latency/cost/access control.
- Asks what’s missing vs overkill.

**Reply / addition ideas**
- Add “operational discipline” explicitly: data ownership, change management, and eval drift as sources evolve.
- Push observability: retrieval traces, attribution, and offline/online evaluation loops (golden queries + regression dashboards).
- Highlight ACL pitfalls (pre-filter vs post-filter), multi-tenancy, caching, and safe rollout/rollback strategies for indexes and rerankers.

## TabPFN deployment via AWS SageMaker Marketplace (vendor post)

Link: `https://old.reddit.com/r/mlops/comments/1q8cmpn/tabpfn_deployment_via_aws_sagemaker_marketplace/`

**What it’s about**
- Vendor announcement: TabPFN-2.5 on SageMaker Marketplace to keep inference inside VPC; claims strong performance vs AutoGluon (4-hour tuned) for certain tabular sizes (≤50k rows, ≤2k features).

**Reply / addition ideas**
- Ask for concrete latency/cost curves (instance types, batch sizes, concurrency) and cold start behavior.
- Request reproducible benchmark setup + constraints (feature types, missingness, class imbalance).
- Ask about production considerations: monitoring, drift detection, model updates/versioning, and governance/compliance story.

## Transitioning to MLOps after a career break (career advice)

Link: `https://old.reddit.com/r/mlops/comments/1q7ymli/looking_for_advice_transitioning_to_mlops_after/`

**What it’s about**
- CV/DL background; 2-year break; difficulty getting callbacks; built an end-to-end personal project but not “in production”.
- Open to internships/contract roles; wants advice to break the loop.

**Reply / addition ideas**
- Make the portfolio read “production-like”: CI, infra-as-code, load testing, dashboards, runbooks, and explicit SLOs/cost numbers.
- Tailor to adjacent titles: ML platform engineer, applied ML engineer, data/ML infra.
- Turn the gap into a narrative: what you learned/built recently; show recency via shipping and maintenance (issues/PRs in OSS, blog posts, releases).
- Focus resume bullets on outcomes: reliability, latency, cost, observability, incident response ownership.

## “Vibe scraping at scale with AI web agents” (product pitch)

Link: `https://old.reddit.com/r/mlops/comments/1qacmen/vibe_scraping_at_scale_with_ai_web_agents_just/`

**What it’s about**
- Product pitch for agent-based web scraping from a sheet of URLs; claims DOM-only semantic approach “zero hallucinations”; extension + cloud browsers.
- Minimal engagement; one blunt negative reply.

**Reply / addition ideas**
- Ask for measurable reliability: success rate by site type, retry semantics, anti-bot handling, and “site changed” resilience.
- Ask for compliance stance: robots.txt, rate limiting, TOS, audit logs, and customer responsibility boundaries.
- Encourage sharing benchmarks: throughput, cost per 1k pages, extraction accuracy, and determinism/replay tooling.

## [HIRING] ML Engineers / Researchers – LLMs, Agentic Systems, RL

Link: `https://old.reddit.com/r/mlops/comments/1q89k3x/hiring_ml_engineers_researchers_llms_agentic/`

**What it’s about**
- Hiring post for Yardstick: ML engineers/researchers for agentic systems/RL/DSPy; remote/Bengaluru; link to application form.

**Reply / addition ideas**
- Suggest they add: compensation band, visa policy, seniority levels, interview loop, and whether it’s internship vs full-time.

